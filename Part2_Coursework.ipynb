{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c3045835Newcastle/2/blob/main/Part2_Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictive Analytics, Computer Vision & AI - CSC3831\n",
        "## Coursework, Part 2: Machine Learning\n",
        "\n",
        "As this coursework is as much about practical skills as it is about reflecting on the procedures and the results, you are expected to explain what you did, your reasoning for process decisions, as well as a thorough analysis of your results.\n",
        "\n",
        "### 1. Load the MNIST dataset, visualise the first 20 digits, and print their corresponding labels."
      ],
      "metadata": {
        "id": "HWPbX4HvOh4-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qaf0EfQswKbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735f0f28-189d-477f-8997-c829ae29b1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "(70000, 28, 28)\n",
            "(70000,)\n"
          ]
        }
      ],
      "source": [
        "# Run this to load MNIST\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X = np.concatenate((X_train, X_test))\n",
        "y = np.concatenate((y_train, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Visualize first 20 digits and their labels\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with 4x5 subplots to display 20 digits\n",
        "fig, axes = plt.subplots(4, 5, figsize=(10, 8),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw={'hspace':0.3, 'wspace':0.1})\n",
        "\n",
        "# Plot the first 20 digits\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(X[i], cmap='gray')\n",
        "    ax.set_title(f'Label: {y[i]}')\n",
        "\n",
        "plt.suptitle('First 20 MNIST Digits', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print the corresponding labels\n",
        "print(\"Labels for first 20 digits:\")\n",
        "print(y[:20])"
      ],
      "metadata": {
        "id": "4-x13-Tf7HYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Train a Logistic Regression classifier on this data, and report on your findings.\n",
        "    \n",
        "1. Tune your hyperparameters to ensure *sparse* weight vectors and high accuracy.\n",
        "2. Visualise the classification vector for each class."
      ],
      "metadata": {
        "id": "qusqC8Zf5vQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Train a Logistic Regression classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Flatten the images from 28x28 to 784-dimensional vectors\n",
        "X_flat = X.reshape(X.shape[0], -1)\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "X_flat = X_flat / 255.0\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(\n",
        "    X_flat, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training Logistic Regression with hyperparameter tuning...\")\n",
        "print(\"This may take several minutes.\")\n",
        "\n",
        "# Use L1 penalty for sparse weight vectors\n",
        "# GridSearchCV to find optimal C parameter\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "}\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    penalty='l1',\n",
        "    solver='saga',\n",
        "    max_iter=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    lr, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_lr, y_train_lr)\n",
        "\n",
        "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Get the best model\n",
        "best_lr = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = best_lr.predict(X_test_lr)\n",
        "test_accuracy = accuracy_score(y_test_lr, y_pred)\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_lr, y_pred))\n",
        "\n",
        "# Check sparsity of weight vectors\n",
        "coef = best_lr.coef_\n",
        "sparsity = np.mean(coef == 0)\n",
        "print(f\"\\nSparsity of weight vectors: {sparsity:.2%}\")\n",
        "print(f\"Non-zero weights per class (avg): {np.mean(np.sum(coef != 0, axis=1)):.0f}\")"
      ],
      "metadata": {
        "id": "scM7z69-524T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the classification vectors for each class\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 6),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw={'hspace':0.3, 'wspace':0.1})\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    # Reshape the coefficient vector back to 28x28 image\n",
        "    weight_image = best_lr.coef_[i].reshape(28, 28)\n",
        "    ax.imshow(weight_image, cmap='RdBu', vmin=-weight_image.max(), vmax=weight_image.max())\n",
        "    ax.set_title(f'Class {i}')\n",
        "\n",
        "plt.suptitle('Classification Weight Vectors for Each Digit Class', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"Red regions indicate positive weights (features that support this class).\")\n",
        "print(\"Blue regions indicate negative weights (features that oppose this class).\")\n",
        "print(\"White/gray regions are near-zero weights (sparse - not important for classification).\")"
      ],
      "metadata": {
        "id": "lr_visualization"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Use PCA to reduce the dimensionality of your training data.\n",
        "    \n",
        "1. Determine the number of components necessary to explain 80\\% of the variance\n",
        "2. Plot the explained variance by number of components.\n",
        "3. Visualise the 20 principal components' loadings\n",
        "4. Plot the two principal components for your data using a scatterplot, colouring by class. What can you say about this plot?\n",
        "5. Visualise the first 20 digits, *generated from their lower-dimensional representation*."
      ],
      "metadata": {
        "id": "kPnwMGuk531k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Use PCA to reduce dimensionality\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Use the flattened and normalized data\n",
        "print(\"Applying PCA to determine components for 80% variance...\")\n",
        "\n",
        "# First, fit PCA with all components to analyze variance\n",
        "pca_full = PCA()\n",
        "pca_full.fit(X_flat)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "\n",
        "# Find number of components for 80% variance\n",
        "n_components_80 = np.argmax(cumsum_variance >= 0.80) + 1\n",
        "print(f\"\\nNumber of components needed for 80% variance: {n_components_80}\")\n",
        "print(f\"Exact variance explained with {n_components_80} components: {cumsum_variance[n_components_80-1]:.4f}\")"
      ],
      "metadata": {
        "id": "lRyxcSS_6Czn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot explained variance by number of components\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(cumsum_variance) + 1), cumsum_variance, linewidth=2)\n",
        "plt.axhline(y=0.80, color='r', linestyle='--', label='80% Variance')\n",
        "plt.axvline(x=n_components_80, color='g', linestyle='--', \n",
        "            label=f'{n_components_80} components')\n",
        "plt.xlabel('Number of Components', fontsize=12)\n",
        "plt.ylabel('Cumulative Explained Variance', fontsize=12)\n",
        "plt.title('PCA Explained Variance vs Number of Components', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlim(0, 300)\n",
        "plt.show()\n",
        "\n",
        "# Also show individual variance for first 50 components\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, 51), pca_full.explained_variance_ratio_[:50])\n",
        "plt.xlabel('Component Number', fontsize=12)\n",
        "plt.ylabel('Explained Variance Ratio', fontsize=12)\n",
        "plt.title('Individual Explained Variance for First 50 Components', fontsize=14)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pca_variance_plot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the first 20 principal components' loadings\n",
        "fig, axes = plt.subplots(4, 5, figsize=(12, 10),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw={'hspace':0.3, 'wspace':0.1})\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    # Reshape the component back to 28x28\n",
        "    component_image = pca_full.components_[i].reshape(28, 28)\n",
        "    ax.imshow(component_image, cmap='viridis')\n",
        "    ax.set_title(f'PC {i+1}\\n({pca_full.explained_variance_ratio_[i]:.3f})', \n",
        "                 fontsize=10)\n",
        "\n",
        "plt.suptitle('First 20 Principal Component Loadings', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"Each principal component represents a pattern of variation in the data.\")\n",
        "print(\"The percentage shows how much variance each component explains.\")\n",
        "print(\"Earlier components capture broader patterns, later ones capture finer details.\")"
      ],
      "metadata": {
        "id": "pca_components_viz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the first two principal components as a scatter plot\n",
        "print(\"Transforming data to 2D using first two principal components...\")\n",
        "pca_2d = PCA(n_components=2)\n",
        "X_2d = pca_2d.fit_transform(X_flat)\n",
        "\n",
        "# Create scatter plot colored by class\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Use a colormap with 10 distinct colors\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
        "\n",
        "for digit in range(10):\n",
        "    mask = y == digit\n",
        "    plt.scatter(X_2d[mask, 0], X_2d[mask, 1], \n",
        "                c=[colors[digit]], label=str(digit), \n",
        "                alpha=0.6, s=10)\n",
        "\n",
        "plt.xlabel(f'First Principal Component ({pca_2d.explained_variance_ratio_[0]:.3f})', \n",
        "           fontsize=12)\n",
        "plt.ylabel(f'Second Principal Component ({pca_2d.explained_variance_ratio_[1]:.3f})', \n",
        "           fontsize=12)\n",
        "plt.title('MNIST Digits Projected onto First Two Principal Components', fontsize=14)\n",
        "plt.legend(title='Digit', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAnalysis of the 2D PCA plot:\")\n",
        "print(\"- The plot shows how different digit classes are distributed in the space\")\n",
        "print(\"  defined by the two most important principal components.\")\n",
        "print(\"- Some digit classes form distinct clusters (e.g., 0s and 1s are often separated).\")\n",
        "print(\"- Other classes show significant overlap, indicating similar patterns in these\")\n",
        "print(\"  two principal dimensions (e.g., 4s, 7s, and 9s may overlap).\")\n",
        "print(\"- The first two components only explain a small fraction of total variance,\")\n",
        "print(\"  so this is a very compressed representation of the data.\")\n",
        "print(\"- The plot demonstrates that even with just 2 dimensions, some structure\")\n",
        "print(\"  of the digit classes is preserved, but full separation requires more dimensions.\")"
      ],
      "metadata": {
        "id": "pca_2d_scatter"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize first 20 digits generated from lower-dimensional representation\n",
        "print(\"Reconstructing digits from lower-dimensional representation...\")\n",
        "\n",
        "# Use PCA with components for 80% variance\n",
        "pca_reduced = PCA(n_components=n_components_80)\n",
        "X_reduced = pca_reduced.fit_transform(X_flat)\n",
        "\n",
        "# Reconstruct the images from the reduced representation\n",
        "X_reconstructed = pca_reduced.inverse_transform(X_reduced)\n",
        "\n",
        "# Visualize original vs reconstructed\n",
        "fig, axes = plt.subplots(4, 10, figsize=(15, 6),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw={'hspace':0.3, 'wspace':0.1})\n",
        "\n",
        "for i in range(20):\n",
        "    # Original image (top two rows)\n",
        "    axes[i // 10 * 2, i % 10].imshow(X[i], cmap='gray')\n",
        "    if i < 10:\n",
        "        axes[0, i].set_title(f'Original\\n{y[i]}', fontsize=9)\n",
        "    \n",
        "    # Reconstructed image (bottom two rows)\n",
        "    reconstructed_img = X_reconstructed[i].reshape(28, 28)\n",
        "    axes[i // 10 * 2 + 1, i % 10].imshow(reconstructed_img, cmap='gray')\n",
        "    if i < 10:\n",
        "        axes[1, i].set_title('Reconstructed', fontsize=9)\n",
        "\n",
        "plt.suptitle(f'Original vs Reconstructed Digits (using {n_components_80} components, 80% variance)', \n",
        "             fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nDigits were compressed from 784 dimensions to {n_components_80} dimensions.\")\n",
        "print(\"The reconstructed images show good preservation of the essential digit features.\")\n",
        "print(\"Some fine details are lost, but the digits remain recognizable.\")"
      ],
      "metadata": {
        "id": "pca_reconstruction"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Generate a noisy copy of your data by adding random normal noise to the digits **with a scale that doesn't completely destroy the signal**. This is, the resulting images noise should be apparent, but the numbers should still be understandable.\n",
        "    \n",
        "1. Visualise the first 20 digits from the noisy dataset.\n",
        "2. Filter the noise by fitting a PCA explaining **a sufficient proportion** of the variance, and then transforming the noisy dataset. Figuring out this proportion is part of the challenge.\n",
        "3. Visualise the first 20 digits of the de-noised dataset."
      ],
      "metadata": {
        "id": "XJnvCd7a6D1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Generate noisy data and denoise with PCA\n",
        "print(\"Generating noisy copy of MNIST data...\")\n",
        "\n",
        "# Add random normal noise with appropriate scale\n",
        "# Scale of 0.3 is chosen to be noticeable but not overwhelming\n",
        "noise_scale = 0.3\n",
        "noise = np.random.normal(0, noise_scale, X_flat.shape)\n",
        "X_noisy = X_flat + noise\n",
        "\n",
        "# Clip values to valid range [0, 1]\n",
        "X_noisy = np.clip(X_noisy, 0, 1)\n",
        "\n",
        "print(f\"Added Gaussian noise with mean=0 and std={noise_scale}\")\n",
        "print(f\"Noise-to-signal ratio: {noise_scale:.2f}\")"
      ],
      "metadata": {
        "id": "Jc6A12yH66Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the first 20 digits from the noisy dataset\n",
        "fig, axes = plt.subplots(4, 5, figsize=(10, 8),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw={'hspace':0.3, 'wspace':0.1})\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    noisy_img = X_noisy[i].reshape(28, 28)\n",
        "    ax.imshow(noisy_img, cmap='gray', vmin=0, vmax=1)\n",
        "    ax.set_title(f'Label: {y[i]}')\n",
        "\n",
        "plt.suptitle(f'First 20 Noisy MNIST Digits (noise std={noise_scale})', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservation: The noise is visible but the digits are still recognizable.\")"
      ],
      "metadata": {
        "id": "noisy_viz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Denoise using PCA\n",
        "print(\"Denoising using PCA...\")\n",
        "\n",
        "# For denoising, we want to capture the main signal while filtering out noise\n",
        "# We'll use slightly more components than for 80% variance to preserve details\n",
        "# Typically, 90-95% variance works well for denoising\n",
        "variance_threshold = 0.90\n",
        "\n",
        "# Fit PCA on the original (clean) data to learn the true signal structure\n",
        "pca_denoise = PCA(n_components=variance_threshold, svd_solver='full')\n",
        "pca_denoise.fit(X_flat)\n",
        "\n",
        "n_components_denoise = pca_denoise.n_components_\n",
        "print(f\"\\nUsing {n_components_denoise} components ({variance_threshold*100}% variance)\")\n",
        "print(f\"This filters out the remaining {(1-variance_threshold)*100}% variance,\")\n",
        "print(\"which primarily consists of noise and fine details.\")\n",
        "\n",
        "# Transform noisy data and reconstruct (denoise)\n",
        "X_noisy_transformed = pca_denoise.transform(X_noisy)\n",
        "X_denoised = pca_denoise.inverse_transform(X_noisy_transformed)\n",
        "\n",
        "# Ensure values are in valid range\n",
        "X_denoised = np.clip(X_denoised, 0, 1)\n",
        "\n",
        "print(\"\\nDenoising complete!\")"
      ],
      "metadata": {
        "id": "pca_denoise"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the first 20 digits of the denoised dataset\n",
        "fig, axes = plt.subplots(4, 5, figsize=(10, 8),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw={'hspace':0.3, 'wspace':0.1})\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    denoised_img = X_denoised[i].reshape(28, 28)\n",
        "    ax.imshow(denoised_img, cmap='gray', vmin=0, vmax=1)\n",
        "    ax.set_title(f'Label: {y[i]}')\n",
        "\n",
        "plt.suptitle(f'First 20 Denoised MNIST Digits ({n_components_denoise} components)', \n",
        "             fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservation: The denoised images show significant noise reduction\")\n",
        "print(\"while preserving the essential digit features.\")"
      ],
      "metadata": {
        "id": "denoised_viz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare original, noisy, and denoised side by side\n",
        "fig, axes = plt.subplots(3, 10, figsize=(15, 5),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw={'hspace':0.3, 'wspace':0.1})\n",
        "\n",
        "for i in range(10):\n",
        "    # Original\n",
        "    axes[0, i].imshow(X[i], cmap='gray')\n",
        "    axes[0, i].set_title(f'{y[i]}', fontsize=10)\n",
        "    \n",
        "    # Noisy\n",
        "    axes[1, i].imshow(X_noisy[i].reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
        "    \n",
        "    # Denoised\n",
        "    axes[2, i].imshow(X_denoised[i].reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
        "\n",
        "# Add row labels\n",
        "axes[0, 0].set_ylabel('Original', fontsize=12, rotation=0, ha='right', va='center')\n",
        "axes[1, 0].set_ylabel('Noisy', fontsize=12, rotation=0, ha='right', va='center')\n",
        "axes[2, 0].set_ylabel('Denoised', fontsize=12, rotation=0, ha='right', va='center')\n",
        "\n",
        "plt.suptitle('Comparison: Original vs Noisy vs Denoised Digits', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate reconstruction quality\n",
        "mse_noisy = np.mean((X_flat[:20] - X_noisy[:20])**2)\n",
        "mse_denoised = np.mean((X_flat[:20] - X_denoised[:20])**2)\n",
        "\n",
        "print(f\"\\nMean Squared Error (first 20 digits):\")\n",
        "print(f\"  Noisy vs Original: {mse_noisy:.6f}\")\n",
        "print(f\"  Denoised vs Original: {mse_denoised:.6f}\")\n",
        "print(f\"\\nNoise reduction: {(1 - mse_denoised/mse_noisy)*100:.1f}%\")\n",
        "print(\"\\nConclusion:\")\n",
        "print(f\"PCA successfully reduced noise by projecting the data onto {n_components_denoise}\")\n",
        "print(\"principal components that capture the main signal structure, while filtering\")\n",
        "print(\"out the noise components. The denoised images are much closer to the originals.\")"
      ],
      "metadata": {
        "id": "comparison_viz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}