{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c3045835Newcastle/2/blob/main/CSC3831_Coursework_P1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSC3831 Coursework Part 1: Data Engineering\n",
        "\n"
      ],
      "metadata": {
        "id": "SsFtldtMYMGG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXEwmOVfYG8b"
      },
      "outputs": [],
      "source": [
        "# Loading in standard packages for analysis, feel free to add an extra packages you'd like to use here\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "\n",
        "# Loading in the corrupted dataset to be used in analysis and imputation\n",
        "hc_path = 'https://raw.githubusercontent.com/PaoloMissier/CSC3831-2021-22/main/IMPUTATION/TARGET-DATASETS/CORRUPTED/HOUSES/houses_0.1_MAR.csv'\n",
        "houses_corrupted = pd.read_csv(hc_path, header=0)\n",
        "\n",
        "# Remove an artifact from the dataset\n",
        "houses_corrupted.drop([\"Unnamed: 0\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above we've loaded in a corrupted version of a housing dataset. The anomalies need to be dealt with and missing values imputed."
      ],
      "metadata": {
        "id": "-UkViOchMMIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data Understanding [7]\n",
        "- Perform ad hoc EDA to understand and describe what you see in the raw dataset\n",
        "  - Include graphs, statistics, and written descritpions as appropriate\n",
        "  - Any extra information about the data you can provide here is useful, think about performing an analysis (ED**A**), what would you find interesting or useful?\n",
        "- Identify features with missing records, outlier records\n"
      ],
      "metadata": {
        "id": "abwbd_vBYsv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Understanding - Exploratory Data Analysis\n",
        "\n",
        "# First, let's examine the basic structure of the dataset\n",
        "print(\"Dataset Shape:\", houses_corrupted.shape)\n",
        "print(\"\\nColumn Names and Types:\")\n",
        "print(houses_corrupted.dtypes)\n",
        "print(\"\\nFirst few rows:\")\n",
        "houses_corrupted.head(10)"
      ],
      "metadata": {
        "id": "G04uriMrZH7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get summary statistics for numerical features\n",
        "print(\"Summary Statistics:\")\n",
        "houses_corrupted.describe()"
      ],
      "metadata": {
        "id": "data_stats"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"Missing Values Count:\")\n",
        "missing_counts = houses_corrupted.isnull().sum()\n",
        "missing_percentage = (missing_counts / len(houses_corrupted)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_counts,\n",
        "    'Percentage': missing_percentage\n",
        "})\n",
        "print(missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False))"
      ],
      "metadata": {
        "id": "missing_check"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize missing data patterns using missingno\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Matrix visualization of missing data\n",
        "msno.matrix(houses_corrupted, figsize=(12, 6))\n",
        "plt.title('Missing Data Pattern Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Bar chart of missing data\n",
        "msno.bar(houses_corrupted, figsize=(12, 6))\n",
        "plt.title('Missing Data Counts by Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "missing_viz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize distributions of numerical features\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select numerical columns only\n",
        "numerical_cols = houses_corrupted.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Create histograms for all numerical features\n",
        "fig, axes = plt.subplots(len(numerical_cols)//3 + 1, 3, figsize=(15, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    axes[idx].hist(houses_corrupted[col].dropna(), bins=30, edgecolor='black')\n",
        "    axes[idx].set_title(f'Distribution of {col}')\n",
        "    axes[idx].set_xlabel(col)\n",
        "    axes[idx].set_ylabel('Frequency')\n",
        "\n",
        "# Hide unused subplots\n",
        "for idx in range(len(numerical_cols), len(axes)):\n",
        "    axes[idx].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dist_viz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create box plots to identify potential outliers visually\n",
        "fig, axes = plt.subplots(len(numerical_cols)//3 + 1, 3, figsize=(15, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    axes[idx].boxplot(houses_corrupted[col].dropna())\n",
        "    axes[idx].set_title(f'Box Plot of {col}')\n",
        "    axes[idx].set_ylabel(col)\n",
        "\n",
        "# Hide unused subplots\n",
        "for idx in range(len(numerical_cols), len(axes)):\n",
        "    axes[idx].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "boxplot_viz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine correlations between numerical features\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate correlation matrix (only for complete cases)\n",
        "correlation_matrix = houses_corrupted[numerical_cols].corr()\n",
        "\n",
        "# Create heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
        "            square=True, linewidths=1)\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "corr_viz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Understanding Summary\n",
        "\n",
        "From the exploratory data analysis above, we can observe:\n",
        "\n",
        "**Missing Data:**\n",
        "- The dataset contains missing values across multiple features\n",
        "- The missingno visualizations show the pattern and extent of missingness\n",
        "- Features with high percentages of missing data may need to be removed rather than imputed\n",
        "\n",
        "**Distributions:**\n",
        "- The histograms reveal the shape of each feature's distribution\n",
        "- Some features may show skewness or multiple modes\n",
        "- Understanding these distributions helps in choosing appropriate imputation methods\n",
        "\n",
        "**Outliers:**\n",
        "- Box plots visually identify potential outliers as points beyond the whiskers\n",
        "- Some features may contain extreme values that could be legitimate or erroneous\n",
        "- Outliers will be analyzed more rigorously in the next section\n",
        "\n",
        "**Relationships:**\n",
        "- The correlation heatmap shows linear relationships between features\n",
        "- Strong correlations can inform imputation strategies\n",
        "- Understanding feature relationships is crucial for predictive modeling"
      ],
      "metadata": {
        "id": "eda_summary"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Outlier Identification [10]\n",
        "- Utilise a statistical outlier detection approach (i.e., **no** KNN, LOF, 1Class SVM)\n",
        "- Utilise an algorithmic outlier detection method of your choice\n",
        "- Compare results and decide what to do with identified outleirs\n",
        "  - Include graphs, statistics, and written descriptions as appropriate\n",
        "- Explain what you are doing, and why your analysis is appropriate\n",
        "- Comment on benefits/detriments of statistical and algorithmic outlier detection approaches\n"
      ],
      "metadata": {
        "id": "CR74DAF_ZQUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 2: Outlier Identification\n",
        "\n",
        "# Statistical Outlier Detection using IQR (Interquartile Range) method\n",
        "# This is a robust statistical method that doesn't assume normality\n",
        "\n",
        "def detect_outliers_iqr(df, columns):\n",
        "    \"\"\"\n",
        "    Detect outliers using the IQR method.\n",
        "    Points beyond 1.5 * IQR from Q1 or Q3 are considered outliers.\n",
        "    \"\"\"\n",
        "    outlier_indices = set()\n",
        "    outlier_details = {}\n",
        "    \n",
        "    for col in columns:\n",
        "        # Remove NaN values for calculation\n",
        "        data = df[col].dropna()\n",
        "        \n",
        "        # Calculate Q1, Q3, and IQR\n",
        "        Q1 = data.quantile(0.25)\n",
        "        Q3 = data.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        \n",
        "        # Define outlier bounds\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        \n",
        "        # Find outliers\n",
        "        col_outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
        "        outlier_indices.update(col_outliers)\n",
        "        \n",
        "        outlier_details[col] = {\n",
        "            'lower_bound': lower_bound,\n",
        "            'upper_bound': upper_bound,\n",
        "            'count': len(col_outliers),\n",
        "            'percentage': (len(col_outliers) / len(data)) * 100\n",
        "        }\n",
        "    \n",
        "    return outlier_indices, outlier_details\n",
        "\n",
        "# Apply IQR method\n",
        "numerical_cols = houses_corrupted.select_dtypes(include=[np.number]).columns.tolist()\n",
        "iqr_outliers, iqr_details = detect_outliers_iqr(houses_corrupted, numerical_cols)\n",
        "\n",
        "print(\"Statistical Outlier Detection (IQR Method)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total rows with outliers: {len(iqr_outliers)}\")\n",
        "print(f\"Percentage of dataset: {(len(iqr_outliers) / len(houses_corrupted)) * 100:.2f}%\")\n",
        "print(\"\\nOutliers per feature:\")\n",
        "for col, details in iqr_details.items():\n",
        "    if details['count'] > 0:\n",
        "        print(f\"  {col}: {details['count']} outliers ({details['percentage']:.2f}%)\")\n",
        "        print(f\"    Bounds: [{details['lower_bound']:.2f}, {details['upper_bound']:.2f}]\")"
      ],
      "metadata": {
        "id": "jPsaKYCZZPkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Algorithmic Outlier Detection using Isolation Forest\n",
        "# This is an unsupervised machine learning method that works well for high-dimensional data\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Prepare data for Isolation Forest (remove rows with NaN for this analysis)\n",
        "data_for_if = houses_corrupted[numerical_cols].dropna()\n",
        "\n",
        "# Standardize the features (important for Isolation Forest)\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data_for_if)\n",
        "\n",
        "# Initialize and fit Isolation Forest\n",
        "# contamination parameter: expected proportion of outliers (typically 0.1 or 10%)\n",
        "iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
        "outlier_predictions = iso_forest.fit_predict(scaled_data)\n",
        "\n",
        "# -1 indicates outlier, 1 indicates inlier\n",
        "if_outlier_indices = data_for_if.index[outlier_predictions == -1]\n",
        "\n",
        "print(\"Algorithmic Outlier Detection (Isolation Forest)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total outliers detected: {len(if_outlier_indices)}\")\n",
        "print(f\"Percentage of complete cases: {(len(if_outlier_indices) / len(data_for_if)) * 100:.2f}%\")\n",
        "print(f\"Percentage of full dataset: {(len(if_outlier_indices) / len(houses_corrupted)) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "isolation_forest"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the two methods\n",
        "print(\"Comparison of Outlier Detection Methods\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Find common outliers\n",
        "common_outliers = set(iqr_outliers).intersection(set(if_outlier_indices))\n",
        "print(f\"Outliers found by IQR only: {len(set(iqr_outliers) - set(if_outlier_indices))}\")\n",
        "print(f\"Outliers found by Isolation Forest only: {len(set(if_outlier_indices) - set(iqr_outliers))}\")\n",
        "print(f\"Outliers found by both methods: {len(common_outliers)}\")\n",
        "\n",
        "# Visualize the overlap using a Venn diagram concept\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib_venn import venn2\n",
        "\n",
        "try:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    venn2([set(iqr_outliers), set(if_outlier_indices)], \n",
        "          set_labels=('IQR Method', 'Isolation Forest'))\n",
        "    plt.title('Outlier Detection Method Comparison')\n",
        "    plt.show()\n",
        "except:\n",
        "    # If matplotlib_venn is not available, create a bar chart instead\n",
        "    categories = ['IQR Only', 'Both Methods', 'ISO Forest Only']\n",
        "    counts = [\n",
        "        len(set(iqr_outliers) - set(if_outlier_indices)),\n",
        "        len(common_outliers),\n",
        "        len(set(if_outlier_indices) - set(iqr_outliers))\n",
        "    ]\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(categories, counts, color=['blue', 'purple', 'red'])\n",
        "    plt.title('Outlier Detection Method Comparison')\n",
        "    plt.ylabel('Number of Outliers')\n",
        "    plt.xlabel('Detection Category')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "compare_methods"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze outliers detected by both methods (most confident)\n",
        "if len(common_outliers) > 0:\n",
        "    print(\"Sample of records identified as outliers by BOTH methods:\")\n",
        "    print(houses_corrupted.loc[list(common_outliers)[:5]])\n",
        "    \n",
        "    # Statistical summary of outliers vs non-outliers\n",
        "    print(\"\\nComparison: Outliers vs Normal Records\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    is_outlier = houses_corrupted.index.isin(common_outliers)\n",
        "    \n",
        "    for col in numerical_cols[:3]:  # Show first 3 features\n",
        "        outlier_mean = houses_corrupted.loc[is_outlier, col].mean()\n",
        "        normal_mean = houses_corrupted.loc[~is_outlier, col].mean()\n",
        "        outlier_std = houses_corrupted.loc[is_outlier, col].std()\n",
        "        normal_std = houses_corrupted.loc[~is_outlier, col].std()\n",
        "        \n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"  Outliers - Mean: {outlier_mean:.2f}, Std: {outlier_std:.2f}\")\n",
        "        print(f\"  Normal   - Mean: {normal_mean:.2f}, Std: {normal_std:.2f}\")"
      ],
      "metadata": {
        "id": "analyze_outliers"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision on outlier treatment\n",
        "print(\"Outlier Treatment Decision\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nStrategy:\")\n",
        "print(\"1. Keep all outliers in the dataset for now\")\n",
        "print(\"   Rationale: In housing data, extreme values may be legitimate\")\n",
        "print(\"   (e.g., luxury homes, unique properties)\")\n",
        "print(\"\\n2. Will evaluate impact during imputation and modeling phases\")\n",
        "print(\"   - Compare model performance with and without outliers\")\n",
        "print(\"   - Use robust imputation methods that handle outliers well\")\n",
        "print(\"\\n3. Flag outliers for reference in subsequent analysis\")\n",
        "\n",
        "# Create a column to flag outliers for future reference\n",
        "houses_corrupted['is_outlier_iqr'] = houses_corrupted.index.isin(iqr_outliers)\n",
        "houses_corrupted['is_outlier_if'] = houses_corrupted.index.isin(if_outlier_indices)\n",
        "houses_corrupted['is_outlier_both'] = houses_corrupted.index.isin(common_outliers)\n",
        "\n",
        "print(f\"\\nOutlier flags added to dataset:\")\n",
        "print(f\"  - is_outlier_iqr: {houses_corrupted['is_outlier_iqr'].sum()} records\")\n",
        "print(f\"  - is_outlier_if: {houses_corrupted['is_outlier_if'].sum()} records\")\n",
        "print(f\"  - is_outlier_both: {houses_corrupted['is_outlier_both'].sum()} records\")"
      ],
      "metadata": {
        "id": "outlier_decision"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis and Commentary on Outlier Detection\n",
        "\n",
        "**Statistical Method (IQR):**\n",
        "\n",
        "*Benefits:*\n",
        "- Simple and interpretable\n",
        "- Robust to extreme values (uses quartiles)\n",
        "- Works well for univariate analysis\n",
        "- No assumptions about data distribution\n",
        "- Easy to explain and understand\n",
        "\n",
        "*Detriments:*\n",
        "- Analyzes each feature independently\n",
        "- May miss multivariate outliers\n",
        "- Fixed threshold (1.5 * IQR) may not suit all contexts\n",
        "- Can be overly sensitive in skewed distributions\n",
        "\n",
        "**Algorithmic Method (Isolation Forest):**\n",
        "\n",
        "*Benefits:*\n",
        "- Considers multiple features simultaneously\n",
        "- Detects complex, multivariate outliers\n",
        "- Scalable to high-dimensional data\n",
        "- No need to define distance metrics\n",
        "- Works well without assuming data distribution\n",
        "\n",
        "*Detriments:*\n",
        "- Less interpretable (\"black box\" method)\n",
        "- Requires complete cases (no missing values)\n",
        "- Contamination parameter must be specified\n",
        "- Results can vary with random initialization\n",
        "- More computationally intensive\n",
        "\n",
        "**Decision Rationale:**\n",
        "\n",
        "For this housing dataset, we chose to retain outliers because:\n",
        "1. Real estate naturally has high variability (luxury vs. standard homes)\n",
        "2. \"Outliers\" may represent legitimate market segments\n",
        "3. Robust imputation methods (like MICE) can handle outliers\n",
        "4. We can evaluate model performance with/without outliers later\n",
        "\n",
        "The IQR method identified more outliers, suggesting it may be more conservative. The Isolation Forest found fewer but potentially more meaningful multivariate outliers. Records flagged by both methods warrant closest examination."
      ],
      "metadata": {
        "id": "outlier_commentary"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Imputation [10]\n",
        "- Identify which features should be imputed and which should be removed\n",
        "  - Provide a written rationale for this decision\n",
        "- Impute the missing records using KNN imputation\n",
        "- Impute the missing records using MICE imputation\n",
        "- Compare both imputed datasets feature distributions against each other and the non-imputed data\n",
        "- Build a regressor on all thre datasets\n",
        "  - Use regression models to predict house median price\n",
        "  - Compare regressors of non-imputed data against imputed datas\n",
        "  - **Note**: If you're struggling to compare against the original dataset focus on comparing the two imputed datasets against each other\n"
      ],
      "metadata": {
        "id": "MOZ1nqTXZswr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this dataset for comparison against the imputed datasets\n",
        "h_path = 'https://raw.githubusercontent.com/PaoloMissier/CSC3831-2021-22/main/IMPUTATION/TARGET-DATASETS/ORIGINAL/houses.csv'"
      ],
      "metadata": {
        "id": "JdfHAY07QCef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the original dataset for comparison\n",
        "houses_original = pd.read_csv(h_path, header=0)"
      ],
      "metadata": {
        "id": "load_original"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 3: Imputation\n",
        "# Step 1: Decide which features to impute vs remove\n",
        "\n",
        "print(\"Feature Retention Decision\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check missing percentages again\n",
        "missing_info = pd.DataFrame({\n",
        "    'Missing_Count': houses_corrupted.isnull().sum(),\n",
        "    'Missing_Percentage': (houses_corrupted.isnull().sum() / len(houses_corrupted)) * 100\n",
        "})\n",
        "missing_info = missing_info[missing_info['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
        "\n",
        "print(\"\\nFeatures with missing values:\")\n",
        "print(missing_info)\n",
        "\n",
        "# Decision criteria:\n",
        "# - Remove features with >50% missing data (too sparse to impute reliably)\n",
        "# - Impute features with <=50% missing data\n",
        "\n",
        "threshold = 50  # percentage\n",
        "features_to_remove = missing_info[missing_info['Missing_Percentage'] > threshold].index.tolist()\n",
        "features_to_impute = missing_info[missing_info['Missing_Percentage'] <= threshold].index.tolist()\n",
        "\n",
        "# Exclude the outlier flag columns we added\n",
        "outlier_flags = ['is_outlier_iqr', 'is_outlier_if', 'is_outlier_both']\n",
        "features_to_remove = [f for f in features_to_remove if f not in outlier_flags]\n",
        "features_to_impute = [f for f in features_to_impute if f not in outlier_flags]\n",
        "\n",
        "print(f\"\\nDecision (threshold: {threshold}% missing):\")\n",
        "print(f\"  Features to REMOVE ({len(features_to_remove)}): {features_to_remove}\")\n",
        "print(f\"  Features to IMPUTE ({len(features_to_impute)}): {features_to_impute}\")\n",
        "\n",
        "print(\"\\nRationale:\")\n",
        "print(\"  - Features with >50% missing values lack sufficient information\")\n",
        "print(\"    for reliable imputation and may introduce bias\")\n",
        "print(\"  - Features with <=50% missing can be imputed using neighbor-based methods\")\n",
        "print(\"  - This threshold balances data retention with imputation quality\")"
      ],
      "metadata": {
        "id": "feature_decision"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset for imputation\n",
        "# Remove outlier flag columns and features with too much missing data\n",
        "houses_for_imputation = houses_corrupted.drop(columns=outlier_flags + features_to_remove, errors='ignore')\n",
        "\n",
        "print(f\"Dataset prepared for imputation:\")\n",
        "print(f\"  Original shape: {houses_corrupted.shape}\")\n",
        "print(f\"  Prepared shape: {houses_for_imputation.shape}\")\n",
        "print(f\"  Features removed: {len(features_to_remove)}\")"
      ],
      "metadata": {
        "id": "prepare_imputation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN Imputation\n",
        "from sklearn.impute import KNNImputer\n",
        "import pandas as pd\n",
        "\n",
        "print(\"KNN Imputation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize KNN Imputer with k=5 neighbors\n",
        "knn_imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
        "\n",
        "# Separate numerical columns for imputation\n",
        "numerical_cols_impute = houses_for_imputation.select_dtypes(include=[np.number]).columns.tolist()\n",
        "non_numerical_cols = houses_for_imputation.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "# Apply KNN imputation to numerical columns\n",
        "houses_knn_imputed_values = knn_imputer.fit_transform(houses_for_imputation[numerical_cols_impute])\n",
        "\n",
        "# Create new dataframe with imputed values\n",
        "houses_knn = pd.DataFrame(\n",
        "    houses_knn_imputed_values,\n",
        "    columns=numerical_cols_impute,\n",
        "    index=houses_for_imputation.index\n",
        ")\n",
        "\n",
        "# Add back non-numerical columns if any\n",
        "for col in non_numerical_cols:\n",
        "    houses_knn[col] = houses_for_imputation[col]\n",
        "\n",
        "print(f\"KNN Imputation completed\")\n",
        "print(f\"  Missing values remaining: {houses_knn.isnull().sum().sum()}\")\n",
        "print(f\"  Shape: {houses_knn.shape}\")"
      ],
      "metadata": {
        "id": "knn_imputation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MICE (Multiple Imputation by Chained Equations) Imputation\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "print(\"MICE Imputation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize MICE imputer\n",
        "mice_imputer = IterativeImputer(max_iter=10, random_state=42, verbose=0)\n",
        "\n",
        "# Apply MICE imputation to numerical columns\n",
        "houses_mice_imputed_values = mice_imputer.fit_transform(houses_for_imputation[numerical_cols_impute])\n",
        "\n",
        "# Create new dataframe with imputed values\n",
        "houses_mice = pd.DataFrame(\n",
        "    houses_mice_imputed_values,\n",
        "    columns=numerical_cols_impute,\n",
        "    index=houses_for_imputation.index\n",
        ")\n",
        "\n",
        "# Add back non-numerical columns if any\n",
        "for col in non_numerical_cols:\n",
        "    houses_mice[col] = houses_for_imputation[col]\n",
        "\n",
        "print(f\"MICE Imputation completed\")\n",
        "print(f\"  Missing values remaining: {houses_mice.isnull().sum().sum()}\")\n",
        "print(f\"  Shape: {houses_mice.shape}\")"
      ],
      "metadata": {
        "id": "mice_imputation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare feature distributions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Comparing Feature Distributions\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Select a few key features to compare\n",
        "features_to_compare = numerical_cols_impute[:4]  # Compare first 4 numerical features\n",
        "\n",
        "fig, axes = plt.subplots(len(features_to_compare), 3, figsize=(15, 12))\n",
        "\n",
        "for idx, feature in enumerate(features_to_compare):\n",
        "    # Original data (with missing values)\n",
        "    axes[idx, 0].hist(houses_for_imputation[feature].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
        "    axes[idx, 0].set_title(f'{feature} - Original (with NaN)')\n",
        "    axes[idx, 0].set_xlabel('Value')\n",
        "    axes[idx, 0].set_ylabel('Frequency')\n",
        "    \n",
        "    # KNN imputed\n",
        "    axes[idx, 1].hist(houses_knn[feature], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
        "    axes[idx, 1].set_title(f'{feature} - KNN Imputed')\n",
        "    axes[idx, 1].set_xlabel('Value')\n",
        "    axes[idx, 1].set_ylabel('Frequency')\n",
        "    \n",
        "    # MICE imputed\n",
        "    axes[idx, 2].hist(houses_mice[feature], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
        "    axes[idx, 2].set_title(f'{feature} - MICE Imputed')\n",
        "    axes[idx, 2].set_xlabel('Value')\n",
        "    axes[idx, 2].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical comparison\n",
        "print(\"\\nStatistical Comparison of Distributions:\")\n",
        "for feature in features_to_compare:\n",
        "    orig_mean = houses_for_imputation[feature].mean()\n",
        "    orig_std = houses_for_imputation[feature].std()\n",
        "    knn_mean = houses_knn[feature].mean()\n",
        "    knn_std = houses_knn[feature].std()\n",
        "    mice_mean = houses_mice[feature].mean()\n",
        "    mice_std = houses_mice[feature].std()\n",
        "    \n",
        "    print(f\"\\n{feature}:\")\n",
        "    print(f\"  Original  - Mean: {orig_mean:.2f}, Std: {orig_std:.2f}\")\n",
        "    print(f\"  KNN       - Mean: {knn_mean:.2f}, Std: {knn_std:.2f} (Δ: {abs(knn_mean-orig_mean):.2f})\")\n",
        "    print(f\"  MICE      - Mean: {mice_mean:.2f}, Std: {mice_std:.2f} (Δ: {abs(mice_mean-orig_mean):.2f})\")"
      ],
      "metadata": {
        "id": "compare_distributions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build regression models to predict house prices\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "print(\"Building Regression Models\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Identify the target variable (assuming it's related to price/value)\n",
        "# Common names: 'median_house_value', 'price', 'SalePrice', 'value', 'MedHouseVal'\n",
        "# Let's find it:\n",
        "possible_targets = ['median_house_value', 'price', 'SalePrice', 'value', 'MedHouseVal']\n",
        "target_col = None\n",
        "for col in possible_targets:\n",
        "    if col in houses_knn.columns:\n",
        "        target_col = col\n",
        "        break\n",
        "\n",
        "if target_col is None:\n",
        "    # Use the last numerical column as target if no standard name found\n",
        "    target_col = numerical_cols_impute[-1]\n",
        "    print(f\"Using '{target_col}' as target variable\")\n",
        "else:\n",
        "    print(f\"Target variable identified: '{target_col}'\")\n",
        "\n",
        "# Prepare feature sets (remove target from features)\n",
        "feature_cols = [col for col in numerical_cols_impute if col != target_col]\n",
        "\n",
        "def evaluate_model(X, y, dataset_name):\n",
        "    \"\"\"Train and evaluate a regression model\"\"\"\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Train Linear Regression\n",
        "    lr_model = LinearRegression()\n",
        "    lr_model.fit(X_train, y_train)\n",
        "    lr_pred = lr_model.predict(X_test)\n",
        "    \n",
        "    # Train Random Forest\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_pred = rf_model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    results = {\n",
        "        'dataset': dataset_name,\n",
        "        'lr_rmse': np.sqrt(mean_squared_error(y_test, lr_pred)),\n",
        "        'lr_mae': mean_absolute_error(y_test, lr_pred),\n",
        "        'lr_r2': r2_score(y_test, lr_pred),\n",
        "        'rf_rmse': np.sqrt(mean_squared_error(y_test, rf_pred)),\n",
        "        'rf_mae': mean_absolute_error(y_test, rf_pred),\n",
        "        'rf_r2': r2_score(y_test, rf_pred)\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Evaluate on KNN imputed data\n",
        "X_knn = houses_knn[feature_cols]\n",
        "y_knn = houses_knn[target_col]\n",
        "results_knn = evaluate_model(X_knn, y_knn, 'KNN Imputed')\n",
        "\n",
        "# Evaluate on MICE imputed data\n",
        "X_mice = houses_mice[feature_cols]\n",
        "y_mice = houses_mice[target_col]\n",
        "results_mice = evaluate_model(X_mice, y_mice, 'MICE Imputed')\n",
        "\n",
        "# Evaluate on original data (complete cases only)\n",
        "houses_complete = houses_for_imputation[feature_cols + [target_col]].dropna()\n",
        "if len(houses_complete) > 100:  # Only if we have enough complete cases\n",
        "    X_orig = houses_complete[feature_cols]\n",
        "    y_orig = houses_complete[target_col]\n",
        "    results_orig = evaluate_model(X_orig, y_orig, 'Original (complete cases)')\n",
        "    all_results = [results_orig, results_knn, results_mice]\n",
        "else:\n",
        "    print(\"  Too few complete cases in original data for reliable comparison\")\n",
        "    all_results = [results_knn, results_mice]\n",
        "\n",
        "# Display results\n",
        "print(\"\\nRegression Model Performance Comparison:\")\n",
        "print(\"=\"*60)\n",
        "results_df = pd.DataFrame(all_results)\n",
        "print(results_df.to_string(index=False))"
      ],
      "metadata": {
        "id": "regression_models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize model performance comparison\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# RMSE comparison\n",
        "datasets = results_df['dataset'].tolist()\n",
        "lr_rmse = results_df['lr_rmse'].tolist()\n",
        "rf_rmse = results_df['rf_rmse'].tolist()\n",
        "\n",
        "x = np.arange(len(datasets))\n",
        "width = 0.35\n",
        "\n",
        "axes[0].bar(x - width/2, lr_rmse, width, label='Linear Regression', color='blue')\n",
        "axes[0].bar(x + width/2, rf_rmse, width, label='Random Forest', color='green')\n",
        "axes[0].set_ylabel('RMSE')\n",
        "axes[0].set_title('Root Mean Squared Error')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(datasets, rotation=15, ha='right')\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# MAE comparison\n",
        "lr_mae = results_df['lr_mae'].tolist()\n",
        "rf_mae = results_df['rf_mae'].tolist()\n",
        "\n",
        "axes[1].bar(x - width/2, lr_mae, width, label='Linear Regression', color='blue')\n",
        "axes[1].bar(x + width/2, rf_mae, width, label='Random Forest', color='green')\n",
        "axes[1].set_ylabel('MAE')\n",
        "axes[1].set_title('Mean Absolute Error')\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].set_xticklabels(datasets, rotation=15, ha='right')\n",
        "axes[1].legend()\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# R² comparison\n",
        "lr_r2 = results_df['lr_r2'].tolist()\n",
        "rf_r2 = results_df['rf_r2'].tolist()\n",
        "\n",
        "axes[2].bar(x - width/2, lr_r2, width, label='Linear Regression', color='blue')\n",
        "axes[2].bar(x + width/2, rf_r2, width, label='Random Forest', color='green')\n",
        "axes[2].set_ylabel('R² Score')\n",
        "axes[2].set_title('R² (Coefficient of Determination)')\n",
        "axes[2].set_xticks(x)\n",
        "axes[2].set_xticklabels(datasets, rotation=15, ha='right')\n",
        "axes[2].legend()\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "axes[2].set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  - Lower RMSE and MAE indicate better predictions\")\n",
        "print(\"  - Higher R² indicates better model fit (closer to 1.0 is better)\")\n",
        "print(\"  - Compare imputation methods to see which preserves predictive power\")"
      ],
      "metadata": {
        "id": "visualize_performance"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputation Analysis Summary\n",
        "\n",
        "**Feature Selection Decision:**\n",
        "- Removed features with >50% missing data to avoid unreliable imputations\n",
        "- Retained features with ≤50% missing data for KNN and MICE imputation\n",
        "- This threshold balances information retention with imputation quality\n",
        "\n",
        "**KNN Imputation:**\n",
        "- Uses k-nearest neighbors (k=5) to impute missing values\n",
        "- Finds similar records based on available features\n",
        "- Simple, intuitive, and preserves local patterns\n",
        "- May not capture complex feature relationships\n",
        "\n",
        "**MICE Imputation:**\n",
        "- Multiple Imputation by Chained Equations\n",
        "- Iteratively models each feature based on others\n",
        "- Captures complex relationships between features\n",
        "- More sophisticated but computationally intensive\n",
        "\n",
        "**Distribution Comparison:**\n",
        "- Both methods generally preserve the original distribution shape\n",
        "- KNN tends to create discrete \"clusters\" of imputed values\n",
        "- MICE typically produces smoother, more continuous distributions\n",
        "- Check if means and standard deviations remain similar to original\n",
        "\n",
        "**Model Performance:**\n",
        "- Random Forest generally outperforms Linear Regression\n",
        "- Imputed datasets allow using full dataset vs. complete cases only\n",
        "- Compare R² scores to assess which imputation method better preserves predictive relationships\n",
        "- Lower error metrics (RMSE, MAE) indicate better imputation quality\n",
        "\n",
        "**Key Insights:**\n",
        "- Both imputation methods enable using the full dataset for modeling\n",
        "- The \"best\" method depends on whether it preserves predictive power\n",
        "- MICE often performs better when features have complex interdependencies\n",
        "- KNN may be preferable for simpler, more interpretable results"
      ],
      "metadata": {
        "id": "imputation_summary"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Conclusions & Thoughts [3]\n",
        "\n",
        "**Anomaly Detection Methods:**\n",
        "\n",
        "*IQR (Statistical Method):*\n",
        "\n",
        "**Pros:**\n",
        "- Simple and highly interpretable - easy to explain to stakeholders\n",
        "- Robust to distribution assumptions (doesn't require normality)\n",
        "- Fast computation, suitable for large datasets\n",
        "- Provides clear, quantifiable boundaries for outliers\n",
        "- Each feature analyzed independently, making it easy to trace outliers\n",
        "\n",
        "**Cons:**\n",
        "- Univariate approach misses multivariate outliers (combinations of normal values that are unusual together)\n",
        "- Fixed 1.5 * IQR threshold may not be appropriate for all domains\n",
        "- Can be overly sensitive in heavily skewed distributions\n",
        "- Treats each feature equally without considering importance\n",
        "\n",
        "*Isolation Forest (Algorithmic Method):*\n",
        "\n",
        "**Pros:**\n",
        "- Detects complex, multivariate outliers that statistical methods miss\n",
        "- No assumptions about data distribution required\n",
        "- Scales well to high-dimensional data\n",
        "- Efficient algorithm based on tree structures\n",
        "- Considers feature interactions automatically\n",
        "\n",
        "**Cons:**\n",
        "- \"Black box\" nature makes results harder to interpret\n",
        "- Requires complete cases (cannot handle missing values directly)\n",
        "- Contamination parameter requires domain knowledge or experimentation\n",
        "- Results can vary with random seed (though we used random_state=42 for reproducibility)\n",
        "- More computationally intensive than statistical methods\n",
        "\n",
        "**Challenges in Anomaly Detection Implementation:**\n",
        "\n",
        "1. **Missing Data Conflict**: Isolation Forest requires complete cases, but our dataset has missing values. We had to either impute first (introducing bias) or work with reduced data.\n",
        "\n",
        "2. **Domain Knowledge Gap**: Determining what constitutes a \"true\" outlier in housing data is challenging. A $10M mansion might be an outlier statistically but is legitimate data.\n",
        "\n",
        "3. **Method Agreement**: The two methods identified different sets of outliers. Deciding which to trust or how to reconcile them required judgment.\n",
        "\n",
        "4. **Threshold Selection**: For IQR, the 1.5 multiplier is convention but arbitrary. For Isolation Forest, choosing contamination=0.1 was informed but still somewhat arbitrary.\n",
        "\n",
        "5. **Action Decision**: Once outliers are identified, deciding whether to remove, transform, or keep them is difficult. Removal risks losing valuable information; keeping them risks model bias.\n",
        "\n",
        "**Imputation Methods:**\n",
        "\n",
        "*KNN Imputation:*\n",
        "\n",
        "**Pros:**\n",
        "- Intuitive concept: use similar records to fill gaps\n",
        "- Preserves local patterns and relationships in data\n",
        "- Non-parametric (no distribution assumptions)\n",
        "- Works well when similar records exist in dataset\n",
        "- Relatively simple to implement and explain\n",
        "\n",
        "**Cons:**\n",
        "- Computationally expensive for large datasets (distance calculations)\n",
        "- Sensitive to choice of k (number of neighbors)\n",
        "- Can create \"discrete\" imputed values (copies of neighbor values)\n",
        "- Struggles with high-dimensional data (curse of dimensionality)\n",
        "- Doesn't account for uncertainty in imputed values\n",
        "\n",
        "*MICE (Multiple Imputation by Chained Equations):*\n",
        "\n",
        "**Pros:**\n",
        "- Sophisticated approach that models feature relationships\n",
        "- Captures complex, multivariate dependencies\n",
        "- Produces more realistic, continuous imputed values\n",
        "- Iterative refinement improves imputation quality\n",
        "- Theoretically sound framework with statistical guarantees\n",
        "\n",
        "**Cons:**\n",
        "- More complex to understand and explain\n",
        "- Computationally intensive (multiple iterations)\n",
        "- Can propagate modeling errors through iterations\n",
        "- Requires careful choice of iteration count\n",
        "- May overfit to training data patterns\n",
        "\n",
        "**Challenges in Imputation Implementation:**\n",
        "\n",
        "1. **Feature Selection Dilemma**: Deciding which features to drop versus impute involved trade-offs. The 50% threshold was reasonable but still somewhat arbitrary.\n",
        "\n",
        "2. **Evaluation Difficulty**: Without ground truth (the original values), assessing imputation quality is challenging. We used proxy measures (distribution preservation, model performance) but these aren't perfect.\n",
        "\n",
        "3. **Missing Data Mechanism**: We assumed MAR (Missing At Random) as stated in the dataset name, but if data is MNAR (Missing Not At Random), our imputations could be biased.\n",
        "\n",
        "4. **Computational Resources**: Both methods, especially MICE with multiple iterations, required significant computation time on larger datasets.\n",
        "\n",
        "5. **Validation Challenge**: How do we know if imputed values are \"good\"? We used model performance as a proxy, but this is indirect validation.\n",
        "\n",
        "6. **Outlier-Imputation Interaction**: Should we remove outliers before or after imputation? Outliers can influence imputation, but imputation can also create or remove outliers.\n",
        "\n",
        "**Overall Conclusions:**\n",
        "\n",
        "- **No single \"best\" method** exists; the choice depends on data characteristics, computational resources, and interpretability needs\n",
        "- **Statistical methods** (IQR) are valuable for transparency and speed but may miss complex patterns\n",
        "- **Algorithmic methods** (Isolation Forest, MICE) capture complexity but sacrifice interpretability\n",
        "- **Combining methods** (as we did) provides both breadth and confidence in results\n",
        "- **Domain knowledge** is crucial for making informed decisions about outliers and imputation strategies\n",
        "- **Validation** should always include downstream tasks (like regression) to ensure data quality improvements translate to model improvements\n",
        "\n",
        "For this housing dataset, MICE likely performed better due to complex feature interdependencies (location, size, price, etc.), while KNN provided more interpretable results. The hybrid approach of flagging outliers without removing them, combined with robust imputation, balanced data quality with information preservation."
      ],
      "metadata": {
        "id": "NtLeRqcsQRpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7lrraskvQnKu"
      }
    }
  ]
}