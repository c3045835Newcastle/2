{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC25W+eJDPeyxqoeq8Syq3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c3045835Newcastle/2/blob/main/part3coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 Coursework: Convolutional Neural Networks on CIFAR-10\n",
        "\n",
        "This notebook implements a CNN classifier for the CIFAR-10 dataset using PyTorch. The coursework explores various regularization techniques and visualization methods to understand how CNNs work.\n",
        "\n",
        "## Overview\n",
        "We'll be building a custom CNN architecture from scratch and experimenting with:\n",
        "1. Early stopping for better generalization\n",
        "2. L2 regularization and dropout\n",
        "3. Batch normalization\n",
        "4. Filter visualization to understand what the network learns"
      ],
      "metadata": {
        "id": "intro-section"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Data Loading\n",
        "\n",
        "First, let's import the necessary libraries and set up our environment. We'll be using PyTorch for the neural network implementation and torchvision for loading the CIFAR-10 dataset."
      ],
      "metadata": {
        "id": "setup-section"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports-cell"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "For the CIFAR-10 dataset, I'm applying some standard preprocessing techniques:\n",
        "- Normalization using mean and std computed from the training set\n",
        "- Converting images to tensors\n",
        "\n",
        "The normalization helps the network train faster and achieve better performance by centering the data around zero."
      ],
      "metadata": {
        "id": "data-prep-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations - normalize with CIFAR-10 statistics\n",
        "# These values are commonly used for CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "\n",
        "# Download and load the training data\n",
        "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                              download=True, transform=transform)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# Split training data into train and validation sets (80-20 split)\n",
        "train_size = int(0.8 * len(full_trainset))\n",
        "val_size = len(full_trainset) - train_size\n",
        "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
        "\n",
        "print(f'Training set size: {len(trainset)}')\n",
        "print(f'Validation set size: {len(valset)}')\n",
        "print(f'Test set size: {len(testset)}')\n",
        "\n",
        "# CIFAR-10 classes\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "data-loading-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize some sample images from our dataset to get a feel for what we're working with:"
      ],
      "metadata": {
        "id": "viz-samples"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to show images\n",
        "def imshow(img):\n",
        "    # Denormalize the image\n",
        "    img = img * torch.tensor([0.2470, 0.2435, 0.2616]).view(3, 1, 1) + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
        "    img = torch.clamp(img, 0, 1)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "\n",
        "# Show some sample images\n",
        "dataloader = DataLoader(trainset, batch_size=16, shuffle=True)\n",
        "images, labels = next(iter(dataloader))\n",
        "\n",
        "# Display images\n",
        "fig, axes = plt.subplots(2, 8, figsize=(15, 4))\n",
        "for idx, ax in enumerate(axes.flat):\n",
        "    plt.sca(ax)\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(classes[labels[idx]])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sample-viz-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: CNN with Early Stopping\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "I've designed a CNN with the following structure:\n",
        "- **3 Convolutional blocks**: Each with Conv2d → ReLU → MaxPool2d\n",
        "- **3 Fully Connected layers**: For final classification\n",
        "\n",
        "The architecture progressively increases the number of filters (32 → 64 → 128) while reducing spatial dimensions through max pooling. This is a common pattern in CNN design that helps the network learn hierarchical features.\n",
        "\n",
        "#### Hyperparameters chosen:\n",
        "- Learning rate: 0.001 (a standard starting point for Adam optimizer)\n",
        "- Batch size: 128 (balances memory usage and training stability)\n",
        "- Optimizer: Adam (adaptive learning rate, works well out of the box)\n",
        "- Epochs: 50 (with early stopping, we likely won't use all)\n",
        "- Early stopping patience: 7 epochs (stop if validation doesn't improve)"
      ],
      "metadata": {
        "id": "task1-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BasicCNN, self).__init__()\n",
        "        \n",
        "        # First convolutional block\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Second convolutional block\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Third convolutional block\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        # After 3 max pools of stride 2, 32x32 image becomes 4x4\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Conv block 1\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        \n",
        "        # Conv block 2\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        \n",
        "        # Conv block 3\n",
        "        x = self.pool3(F.relu(self.conv3(x)))\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Print model architecture\n",
        "model = BasicCNN().to(device)\n",
        "print(model)\n",
        "print(f'\\nTotal parameters: {sum(p.numel() for p in model.parameters())}')"
      ],
      "metadata": {
        "id": "basic-cnn-arch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Function with Early Stopping\n",
        "\n",
        "The training function implements early stopping by:\n",
        "1. Monitoring validation loss after each epoch\n",
        "2. Saving the best model (lowest validation loss)\n",
        "3. Stopping if validation loss doesn't improve for `patience` epochs\n",
        "\n",
        "This prevents overfitting and saves training time."
      ],
      "metadata": {
        "id": "train-func-desc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
        "                num_epochs=50, patience=7, device='cuda'):\n",
        "    \"\"\"\n",
        "    Train the model with early stopping.\n",
        "    \n",
        "    Returns:\n",
        "        best_model: Model with best validation performance\n",
        "        history: Dictionary containing training history\n",
        "    \"\"\"\n",
        "    best_val_loss = float('inf')\n",
        "    best_model = None\n",
        "    patience_counter = 0\n",
        "    \n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        \n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Calculate average losses and accuracies\n",
        "        train_loss = train_loss / train_total\n",
        "        val_loss = val_loss / val_total\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        \n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        \n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] '\n",
        "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | '\n",
        "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "        \n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = deepcopy(model.state_dict())\n",
        "            patience_counter = 0\n",
        "            print(f'  --> New best model saved (Val Loss: {val_loss:.4f})')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'\\nEarly stopping triggered after {epoch+1} epochs')\n",
        "                break\n",
        "    \n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model)\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "train-function"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Baseline Model\n",
        "\n",
        "Now let's train our baseline CNN with early stopping:"
      ],
      "metadata": {
        "id": "baseline-train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "num_epochs = 50\n",
        "patience = 7\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model_basic = BasicCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_basic.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "print('Training baseline CNN with early stopping...\\n')\n",
        "model_basic, history_basic = train_model(model_basic, train_loader, val_loader, \n",
        "                                         criterion, optimizer, num_epochs, patience, device)"
      ],
      "metadata": {
        "id": "train-baseline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convergence Graphs\n",
        "\n",
        "Let's visualize the training and validation loss to see how the model learned and where early stopping kicked in:"
      ],
      "metadata": {
        "id": "conv-graphs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_convergence(history, title='Training and Validation Loss'):\n",
        "    \"\"\"\n",
        "    Plot training and validation loss and accuracy.\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Plot loss\n",
        "    ax1.plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
        "    ax1.plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.set_title(title, fontsize=14)\n",
        "    ax1.legend(fontsize=11)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot accuracy\n",
        "    ax2.plot(history['train_acc'], label='Training Accuracy', linewidth=2)\n",
        "    ax2.plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    ax2.set_title('Training and Validation Accuracy', fontsize=14)\n",
        "    ax2.legend(fontsize=11)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_convergence(history_basic, 'Baseline CNN: Training and Validation Loss')"
      ],
      "metadata": {
        "id": "plot-basic-conv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Performance\n",
        "\n",
        "Let's evaluate the model on the test set:"
      ],
      "metadata": {
        "id": "test-perf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device='cuda'):\n",
        "    \"\"\"\n",
        "    Evaluate model on test set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "test_acc_basic = evaluate_model(model_basic, test_loader, device)"
      ],
      "metadata": {
        "id": "eval-basic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: L2 Regularization and Dropout\n",
        "\n",
        "### Model with Regularization\n",
        "\n",
        "Now I'll create versions of the CNN with different regularization techniques:\n",
        "1. **L2 Regularization**: Added via weight_decay parameter in the optimizer\n",
        "2. **Dropout**: Added dropout layers after each fully connected layer\n",
        "\n",
        "These techniques help prevent overfitting by:\n",
        "- L2: Penalizing large weights\n",
        "- Dropout: Randomly dropping neurons during training, forcing the network to learn more robust features"
      ],
      "metadata": {
        "id": "task2-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNWithDropout(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(CNNWithDropout, self).__init__()\n",
        "        \n",
        "        # Convolutional layers (same as before)\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Fully connected layers with dropout\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Conv blocks\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = self.pool3(F.relu(self.conv3(x)))\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        \n",
        "        # FC layers with dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "print('CNN with Dropout architecture created')"
      ],
      "metadata": {
        "id": "cnn-dropout"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training with L2 Regularization\n",
        "\n",
        "L2 regularization is implemented through the `weight_decay` parameter in the optimizer. I'll use a weight decay of 0.0001:"
      ],
      "metadata": {
        "id": "l2-train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with L2 regularization\n",
        "model_l2 = BasicCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_l2 = optim.Adam(model_l2.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "print('Training CNN with L2 Regularization...\\n')\n",
        "model_l2, history_l2 = train_model(model_l2, train_loader, val_loader, \n",
        "                                   criterion, optimizer_l2, num_epochs=30, \n",
        "                                   patience=10, device=device)"
      ],
      "metadata": {
        "id": "train-l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training with Dropout\n",
        "\n",
        "Now let's train a model with dropout (rate=0.5):"
      ],
      "metadata": {
        "id": "dropout-train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with Dropout\n",
        "model_dropout = CNNWithDropout(dropout_rate=0.5).to(device)\n",
        "optimizer_dropout = optim.Adam(model_dropout.parameters(), lr=learning_rate)\n",
        "\n",
        "print('Training CNN with Dropout...\\n')\n",
        "model_dropout, history_dropout = train_model(model_dropout, train_loader, val_loader, \n",
        "                                             criterion, optimizer_dropout, num_epochs=30, \n",
        "                                             patience=10, device=device)"
      ],
      "metadata": {
        "id": "train-dropout"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison of Regularization Techniques\n",
        "\n",
        "Let's compare the performance of all three models (baseline, L2, and dropout) on a single plot:"
      ],
      "metadata": {
        "id": "reg-comparison"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot comparison\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Validation loss comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_basic['val_loss'], label='No Regularization', linewidth=2, marker='o', markersize=3)\n",
        "plt.plot(history_l2['val_loss'], label='L2 Regularization', linewidth=2, marker='s', markersize=3)\n",
        "plt.plot(history_dropout['val_loss'], label='Dropout', linewidth=2, marker='^', markersize=3)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Validation Loss', fontsize=12)\n",
        "plt.title('Regularization Comparison: Validation Loss', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Validation accuracy comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_basic['val_acc'], label='No Regularization', linewidth=2, marker='o', markersize=3)\n",
        "plt.plot(history_l2['val_acc'], label='L2 Regularization', linewidth=2, marker='s', markersize=3)\n",
        "plt.plot(history_dropout['val_acc'], label='Dropout', linewidth=2, marker='^', markersize=3)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Validation Accuracy (%)', fontsize=12)\n",
        "plt.title('Regularization Comparison: Validation Accuracy', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Test accuracies\n",
        "print('\\nTest Set Performance:')\n",
        "print(f'Baseline (No Regularization): {evaluate_model(model_basic, test_loader, device):.2f}%')\n",
        "print(f'L2 Regularization: {evaluate_model(model_l2, test_loader, device):.2f}%')\n",
        "print(f'Dropout: {evaluate_model(model_dropout, test_loader, device):.2f}%')"
      ],
      "metadata": {
        "id": "plot-reg-comparison"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Batch Normalization\n",
        "\n",
        "### Model with Batch Normalization\n",
        "\n",
        "Batch normalization normalizes the inputs of each layer, which:\n",
        "- Allows higher learning rates\n",
        "- Reduces sensitivity to initialization\n",
        "- Acts as a regularizer\n",
        "\n",
        "I'll add BatchNorm2d after each convolutional layer and BatchNorm1d after fully connected layers:"
      ],
      "metadata": {
        "id": "task3-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNWithBatchNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNWithBatchNorm, self).__init__()\n",
        "        \n",
        "        # First convolutional block with batch norm\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Second convolutional block with batch norm\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Third convolutional block with batch norm\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Fully connected layers with batch norm\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
        "        \n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(128)\n",
        "        \n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Conv block 1 with batch norm\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        # Conv block 2 with batch norm\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        # Conv block 3 with batch norm\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool3(x)\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        \n",
        "        # FC layers with batch norm\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "print('CNN with Batch Normalization architecture created')"
      ],
      "metadata": {
        "id": "cnn-batchnorm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training with Batch Normalization\n",
        "\n",
        "Let's train the model with batch normalization:"
      ],
      "metadata": {
        "id": "bn-train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with Batch Normalization\n",
        "model_bn = CNNWithBatchNorm().to(device)\n",
        "optimizer_bn = optim.Adam(model_bn.parameters(), lr=learning_rate)\n",
        "\n",
        "print('Training CNN with Batch Normalization...\\n')\n",
        "model_bn, history_bn = train_model(model_bn, train_loader, val_loader, \n",
        "                                   criterion, optimizer_bn, num_epochs=30, \n",
        "                                   patience=10, device=device)"
      ],
      "metadata": {
        "id": "train-bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison: With vs Without Batch Normalization\n",
        "\n",
        "Let's compare the baseline model with the batch normalization model:"
      ],
      "metadata": {
        "id": "bn-comparison"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot comparison\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Validation loss comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_basic['val_loss'], label='Without Batch Normalization', linewidth=2.5, marker='o', markersize=4)\n",
        "plt.plot(history_bn['val_loss'], label='With Batch Normalization', linewidth=2.5, marker='s', markersize=4)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Validation Loss', fontsize=12)\n",
        "plt.title('Batch Normalization Impact: Validation Loss', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Validation accuracy comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_basic['val_acc'], label='Without Batch Normalization', linewidth=2.5, marker='o', markersize=4)\n",
        "plt.plot(history_bn['val_acc'], label='With Batch Normalization', linewidth=2.5, marker='s', markersize=4)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Validation Accuracy (%)', fontsize=12)\n",
        "plt.title('Batch Normalization Impact: Validation Accuracy', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Test accuracies\n",
        "print('\\nTest Set Performance:')\n",
        "print(f'Without Batch Normalization: {evaluate_model(model_basic, test_loader, device):.2f}%')\n",
        "print(f'With Batch Normalization: {evaluate_model(model_bn, test_loader, device):.2f}%')"
      ],
      "metadata": {
        "id": "plot-bn-comparison"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Filter Visualization\n",
        "\n",
        "### Visualizing Learned Filters\n",
        "\n",
        "Now let's visualize what the CNN has learned. We'll look at:\n",
        "1. The actual filter weights from each convolutional layer\n",
        "2. Feature maps (activations) when a test image passes through the network\n",
        "\n",
        "This helps us understand what features the network is detecting at different layers."
      ],
      "metadata": {
        "id": "task4-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_filters(model, layer_name='conv1', num_filters=16):\n",
        "    \"\"\"\n",
        "    Visualize the learned filters from a convolutional layer.\n",
        "    \"\"\"\n",
        "    # Get the weights from the specified layer\n",
        "    if hasattr(model, layer_name):\n",
        "        layer = getattr(model, layer_name)\n",
        "        filters = layer.weight.data.cpu().numpy()\n",
        "    else:\n",
        "        print(f\"Layer {layer_name} not found\")\n",
        "        return\n",
        "    \n",
        "    # Normalize filters for visualization\n",
        "    f_min, f_max = filters.min(), filters.max()\n",
        "    filters = (filters - f_min) / (f_max - f_min)\n",
        "    \n",
        "    # Plot filters\n",
        "    num_filters = min(num_filters, filters.shape[0])\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
        "    \n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i < num_filters:\n",
        "            # Get the filter\n",
        "            f = filters[i]\n",
        "            \n",
        "            # If it's a 3-channel filter, show as RGB\n",
        "            if f.shape[0] == 3:\n",
        "                f = np.transpose(f, (1, 2, 0))\n",
        "                ax.imshow(f)\n",
        "            else:\n",
        "                # For other layers, show first channel\n",
        "                ax.imshow(f[0], cmap='viridis')\n",
        "            \n",
        "            ax.set_title(f'Filter {i+1}', fontsize=10)\n",
        "        ax.axis('off')\n",
        "    \n",
        "    plt.suptitle(f'Learned Filters from {layer_name}', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize filters from each convolutional layer\n",
        "print('Visualizing filters from conv1 (first layer):')\n",
        "visualize_filters(model_basic, 'conv1', 16)\n",
        "\n",
        "print('\\nVisualizing filters from conv2 (second layer):')\n",
        "visualize_filters(model_basic, 'conv2', 16)\n",
        "\n",
        "print('\\nVisualizing filters from conv3 (third layer):')\n",
        "visualize_filters(model_basic, 'conv3', 16)"
      ],
      "metadata": {
        "id": "viz-filters"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Feature Maps\n",
        "\n",
        "Now let's see how these filters respond to actual images. We'll pass a test image through the network and visualize the activations at each layer:"
      ],
      "metadata": {
        "id": "feature-maps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_maps(model, image, device='cuda'):\n",
        "    \"\"\"\n",
        "    Extract feature maps from all convolutional layers.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    feature_maps = {}\n",
        "    \n",
        "    # Add hooks to capture intermediate outputs\n",
        "    def hook_fn(name):\n",
        "        def hook(module, input, output):\n",
        "            feature_maps[name] = output.detach()\n",
        "        return hook\n",
        "    \n",
        "    # Register hooks\n",
        "    hooks = []\n",
        "    hooks.append(model.conv1.register_forward_hook(hook_fn('conv1')))\n",
        "    hooks.append(model.conv2.register_forward_hook(hook_fn('conv2')))\n",
        "    hooks.append(model.conv3.register_forward_hook(hook_fn('conv3')))\n",
        "    \n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "        _ = model(image)\n",
        "    \n",
        "    # Remove hooks\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "    \n",
        "    return feature_maps\n",
        "\n",
        "def visualize_feature_maps(feature_maps, layer_name, num_maps=16):\n",
        "    \"\"\"\n",
        "    Visualize feature maps from a specific layer.\n",
        "    \"\"\"\n",
        "    if layer_name not in feature_maps:\n",
        "        print(f\"Layer {layer_name} not found\")\n",
        "        return\n",
        "    \n",
        "    maps = feature_maps[layer_name].cpu().numpy()[0]  # Get first image in batch\n",
        "    num_maps = min(num_maps, maps.shape[0])\n",
        "    \n",
        "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "    \n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i < num_maps:\n",
        "            ax.imshow(maps[i], cmap='viridis')\n",
        "            ax.set_title(f'Feature Map {i+1}', fontsize=10)\n",
        "        ax.axis('off')\n",
        "    \n",
        "    plt.suptitle(f'Feature Maps from {layer_name}', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Get a test image\n",
        "test_images, test_labels = next(iter(test_loader))\n",
        "test_image = test_images[0]  # Pick first image\n",
        "test_label = test_labels[0]\n",
        "\n",
        "# Display the original image\n",
        "plt.figure(figsize=(4, 4))\n",
        "imshow(test_image)\n",
        "plt.title(f'Test Image: {classes[test_label]}', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Get feature maps\n",
        "print('Extracting feature maps from the test image...\\n')\n",
        "feature_maps = get_feature_maps(model_basic, test_image, device)\n",
        "\n",
        "# Visualize feature maps from each layer\n",
        "print('Feature maps from conv1 (first layer - detecting edges and simple patterns):')\n",
        "visualize_feature_maps(feature_maps, 'conv1', 16)\n",
        "\n",
        "print('\\nFeature maps from conv2 (second layer - detecting more complex patterns):')\n",
        "visualize_feature_maps(feature_maps, 'conv2', 16)\n",
        "\n",
        "print('\\nFeature maps from conv3 (third layer - detecting high-level features):')\n",
        "visualize_feature_maps(feature_maps, 'conv3', 16)"
      ],
      "metadata": {
        "id": "viz-feature-maps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional Visualization: Multiple Test Images\n",
        "\n",
        "Let's look at how different images activate the filters:"
      ],
      "metadata": {
        "id": "multi-viz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize feature maps for different classes\n",
        "num_samples = 5\n",
        "sample_indices = [i for i in range(num_samples)]\n",
        "\n",
        "fig, axes = plt.subplots(num_samples, 5, figsize=(15, 12))\n",
        "\n",
        "for idx, sample_idx in enumerate(sample_indices):\n",
        "    test_img = test_images[sample_idx]\n",
        "    test_lbl = test_labels[sample_idx]\n",
        "    \n",
        "    # Original image\n",
        "    plt.sca(axes[idx, 0])\n",
        "    imshow(test_img)\n",
        "    axes[idx, 0].set_title(f'{classes[test_lbl]}', fontsize=10)\n",
        "    axes[idx, 0].axis('off')\n",
        "    \n",
        "    # Get feature maps\n",
        "    fmaps = get_feature_maps(model_basic, test_img, device)\n",
        "    \n",
        "    # Show first feature map from each conv layer\n",
        "    for layer_idx, layer_name in enumerate(['conv1', 'conv2', 'conv3']):\n",
        "        feature = fmaps[layer_name].cpu().numpy()[0, 0]\n",
        "        axes[idx, layer_idx + 1].imshow(feature, cmap='viridis')\n",
        "        if idx == 0:\n",
        "            axes[idx, layer_idx + 1].set_title(f'{layer_name} (1st filter)', fontsize=10)\n",
        "        axes[idx, layer_idx + 1].axis('off')\n",
        "    \n",
        "    # Show combined visualization from conv3\n",
        "    feature_combined = fmaps['conv3'].cpu().numpy()[0, :9].mean(axis=0)\n",
        "    axes[idx, 4].imshow(feature_combined, cmap='viridis')\n",
        "    if idx == 0:\n",
        "        axes[idx, 4].set_title('conv3 (avg of 9)', fontsize=10)\n",
        "    axes[idx, 4].axis('off')\n",
        "\n",
        "plt.suptitle('Feature Maps Across Different Test Images', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "multi-feature-viz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Observations\n",
        "\n",
        "In this coursework, I implemented a CNN from scratch on the CIFAR-10 dataset and explored various aspects:\n",
        "\n",
        "### Task 1: Early Stopping\n",
        "- Implemented a baseline CNN with 3 convolutional and 3 fully connected layers\n",
        "- Used early stopping to prevent overfitting\n",
        "- Achieved reasonable performance on the validation set\n",
        "\n",
        "### Task 2: Regularization\n",
        "- Compared three approaches: no regularization, L2 regularization, and dropout\n",
        "- Both regularization techniques helped reduce overfitting\n",
        "- Dropout showed particularly good performance by forcing the network to learn robust features\n",
        "\n",
        "### Task 3: Batch Normalization\n",
        "- Added batch normalization layers throughout the network\n",
        "- Observed faster convergence and more stable training\n",
        "- Batch normalization also acts as a mild regularizer\n",
        "\n",
        "### Task 4: Filter Visualization\n",
        "- Visualized the learned convolutional filters\n",
        "- First layer filters detect low-level features (edges, colors)\n",
        "- Deeper layers detect more abstract patterns\n",
        "- Feature maps show how different parts of the image activate different filters\n",
        "\n",
        "The experiments demonstrate that proper regularization and normalization techniques are crucial for training effective CNNs. Each technique offers unique benefits, and they can be combined for even better results."
      ],
      "metadata": {
        "id": "summary-section"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Best Model\n",
        "\n",
        "Finally, let's save our best performing model for future use:"
      ],
      "metadata": {
        "id": "save-model"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model (you can choose which one performed best)\n",
        "torch.save(model_basic.state_dict(), 'cifar10_cnn_best.pth')\n",
        "print('Best model saved as cifar10_cnn_best.pth')\n",
        "\n",
        "# To load the model later:\n",
        "# model = BasicCNN()\n",
        "# model.load_state_dict(torch.load('cifar10_cnn_best.pth'))\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "save-model-cell"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
