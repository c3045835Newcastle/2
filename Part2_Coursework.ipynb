{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c3045835Newcastle/2/blob/main/Part2_Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictive Analytics, Computer Vision & AI - CSC3831\n",
        "## Coursework, Part 2: Machine Learning\n",
        "\n",
        "As this coursework is as much about practical skills as it is about reflecting on the procedures and the results, you are expected to explain what you did, your reasoning for process decisions, as well as a thorough analysis of your results.\n",
        "\n",
        "### 1. Load the MNIST dataset, visualise the first 20 digits, and print their corresponding labels."
      ],
      "metadata": {
        "id": "HWPbX4HvOh4-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qaf0EfQswKbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735f0f28-189d-477f-8997-c829ae29b1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "(70000, 28, 28)\n",
            "(70000,)\n"
          ]
        }
      ],
      "source": [
        "# Run this to load MNIST\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X = np.concatenate((X_train, X_test))\n",
        "y = np.concatenate((y_train, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4-x13-Tf7HYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Train a Logistic Regression classifier on this data, and report on your findings.\n",
        "    \n",
        "1. Tune your hyperparameters to ensure *sparse* weight vectors and high accuracy.\n",
        "2. Visualise the classification vector for each class."
      ],
      "metadata": {
        "id": "qusqC8Zf5vQN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "scM7z69-524T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Use PCA to reduce the dimensionality of your training data.\n",
        "    \n",
        "1. Determine the number of components necessary to explain 80\\% of the variance\n",
        "2. Plot the explained variance by number of components.\n",
        "3. Visualise the 20 principal components' loadings\n",
        "4. Plot the two principal components for your data using a scatterplot, colouring by class. What can you say about this plot?\n",
        "5. Visualise the first 20 digits, *generated from their lower-dimensional representation*."
      ],
      "metadata": {
        "id": "kPnwMGuk531k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lRyxcSS_6Czn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Generate a noisy copy of your data by adding random normal noise to the digits **with a scale that doesn't completely destroy the signal**. This is, the resulting images noise should be apparent, but the numbers should still be understandable.\n",
        "    \n",
        "1. Visualise the first 20 digits from the noisy dataset.\n",
        "2. Filter the noise by fitting a PCA explaining **a sufficient proportion** of the variance, and then transforming the noisy dataset. Figuring out this proportion is part of the challenge.\n",
        "3. Visualise the first 20 digits of the de-noised dataset."
      ],
      "metadata": {
        "id": "XJnvCd7a6D1k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jc6A12yH66Dp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}